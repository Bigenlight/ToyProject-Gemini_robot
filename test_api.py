# test_api.py ìˆ˜ì •ë³¸
from google import genai
from google.genai import types

MY_API_KEY = "AIzaSyBpBo1uoFaiwht8jy5VmwVxpCf11aE3bzg"
client = genai.Client(api_key=MY_API_KEY)

def test_gemini():
    try:
        print("ğŸ¤– [System] Geminiì—ê²Œ ì§ˆë¬¸ì„ ë˜ì§€ëŠ” ì¤‘...")
        
        # ì•ˆì „ í•„í„°ë¥¼ ë„ê³  ë‹¤ì‹œ ì‹œë„ (í…ŒìŠ¤íŠ¸ìš©)
        config = types.GenerateContentConfig(
            safety_settings=[
                types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
            ]
        )

        response = client.models.generate_content(
            model="gemini-3-flash-preview", # 3-flash-previewë„ ê°€ëŠ¥
            contents="ì•ˆë…•! ë„ˆëŠ” ë¡œë´‡ íŒ” ì œì–´ ì—ì´ì „íŠ¸ì•¼. ì¤€ë¹„ëë‹ˆ?",
            config=config
        )
        
        # 1. ì‘ë‹µ í…ìŠ¤íŠ¸ í™•ì¸
        if response.text:
            print(f"\nğŸ§  [Gemini]: {response.text}")
        else:
            # 2. í…ìŠ¤íŠ¸ê°€ ì—†ë‹¤ë©´ ì°¨ë‹¨ ì‚¬ìœ  í™•ì¸
            print("\nâš ï¸ [Warning]: í…ìŠ¤íŠ¸ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤.")
            print(f"ì°¨ë‹¨ ì‚¬ìœ (Prompt Feedback): {response.prompt_feedback}")
            print(f"í›„ë³´ ì‘ë‹µ í™•ì¸: {response.candidates[0].finish_reason}")

    except Exception as e:
        print(f"âŒ [Error] ìƒì„¸ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    test_gemini()